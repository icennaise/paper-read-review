1. Introduction

近年来，在NLP系统中出现了预训练语言表征的趋势，并以越来越灵活和与任务无关的方式应用于下游转移。首先，单层表征是使用词向量学习的[MCCD13, PSM14]，并反馈给特定的任务架构，然后具有多层表征和上下文状态的RNN被用来形成更强大的表征[DL15, MBXS17, PNZtY18]（尽管仍然应用于特定的任务架构），最近，预训练的循环或转化器语言模型[VSP+17]已经被直接微调，完全消除对特定任务架构的需求[RNSS18, DCLT18, HR18]。

这最后一种范式在许多具有挑战性的NLP任务上取得了实质性的进展，如阅读理解、问题回答、文本嵌套等，并在新的架构和算法的基础上继续推进[RSR+19, LOG+19, YDY+19, LCG+19] 。然而，这种方法的一个主要限制是，虽然架构是任务无关的，但仍然需要特定任务的数据集和特定任务的微调：要在一个期望的任务上实现强大的性能，通常需要在数千到数十万个特定于该任务的例子的数据集上进行微调。消除这一限制是可取的，原因有几个。

首先，从实用的角度来看，每一项新的任务都需要一个大型的标记例子的数据集，这限制了语言模型的适用性。存在着非常广泛的可能有用的语言任务，包括从纠正语法，到生成抽象概念的例子，再到批评一个短篇故事。对于许多这样的任务来说，收集一个大型的监督训练数据集是很困难的，特别是当这个过程必须为每一个新的任务而重复。

其次，利用训练数据中的虚假关联的可能性从根本上说是随着模型的表现力和训练分布的狭窄程度而增长的。这可能会给预训练加微调模式带来问题，在这种模式下，模型被设计得很大，以便在预训练期间吸收信息，但随后在很窄的任务分布上进行微调。例如，[HLW+20]观察到较大的模型不一定在分布外有更好的泛化。有证据表明，在这种范式下实现的泛化可能很差，因为模型对训练分布过于具体，在分布外的泛化效果不好[YdC+19, MPL19]。因此，微调模型在特定基准上的表现，即使名义上是人类水平，也可能夸大了基础任务的实际表现[GSL+18, NK19]。

第三，人类不需要大量的监督数据集来学习大多数语言任务--自然语言的简短指令（例如 "请告诉我这个句子描述的是快乐的事情还是悲伤的事情"）或最多极少的示范（例如 "这里有两个人们表现勇敢的例子；请给出第三个勇敢的例子"）往往足以使人类执行一项新任务，至少达到合理的能力水平。除了指出我们目前的NLP技术在概念上的局限性外，这种适应性也有实际的好处--它允许人类在许多任务和技能之间无缝混合或切换，例如在冗长的对话中进行加法。为了发挥广泛的作用，我们希望有一天我们的NLP系统也能有这样的流畅性和通用性。

Figure 1.1 : 在无监督的预训练期间，语言模型发展了一套广泛的技能和模式识别能力。然后，它在推理时间使用这些能力来快速适应或识别所需的任务。我们使用术语 "上下文学习 "来描述这个过程的内部循环，它发生在每个序列的前向传递中。本图中的序列并不是为了代表模型在预训练期间看到的数据，而是为了表明有时在一个序列中会有重复的子任务。
Figure 1.2 : 大模型few-shot的效果更好

解决这些问题的一个潜在途径是元学习1--在语言模型的背景下，这意味着模型在训练时发展出一套广泛的技能和模式识别能力，然后在推理时使用这些能力来迅速适应或识别所需的任务（如图1.1所示）。最近的工作[RWC+19]试图通过我们所说的 "语境学习 "来做到这一点，使用预训练的语言模型的文本输入作为任务规范的形式：该模型以自然语言指令和/或任务的几个示范为条件，然后被期望通过预测接下来的内容来完成任务的进一步实例。

虽然它已经显示出一些初步的希望，但这种方法所取得的结果仍然远远不如微调--例如[RWC+19]在自然问题上只取得了4%的成绩，甚至它的55 F1 CoQa结果现在也比技术水平落后35分以上。元学习显然需要大量的改进，以便作为解决语言任务的实用方法而变得可行。

语言建模的另一个最新趋势可能提供了一条前进的道路。近年来，转化器语言模型的容量大幅增加，从1亿个参数[RNSS18]，到3亿个参数[DCLT18]，到15亿个参数[RWC+19]，到80亿个参数[SPP+19]，110亿个参数[RSR+19]，最后到170亿个参数[Tur20]。每一次增加都带来了文本合成和/或下游NLP任务的改进，而且有证据表明，与许多下游任务有很好关联的对数损失，随着规模的扩大也有平滑的改进趋势[KMH+20]。由于上下文学习涉及在模型的参数内吸收许多技能和任务，因此，上下文学习能力可能会随着规模的扩大而显示出类似的强大收益。

在本文中，我们通过训练一个1750亿参数的自回归语言模型（我们称之为GPT-3），并测量其语境中的学习能力来测试这一假设。具体来说，我们在二十多个NLP数据集上评估GPT-3，以及几个旨在测试快速适应不太可能直接包含在训练集中的任务的新任务。对于每个任务，我们在3个条件下评估GPT-3。(a) "少次学习"，即在语境中学习，我们允许在模型的语境窗口中容纳尽可能多的演示（通常是10到100次），(b) "一次学习"，我们只允许一次演示，以及(c) "零次 "学习，不允许演示，只给模型一个自然语言的指令。原则上，GPT-3也可以在传统的微调设置中进行评估，但我们将此留给未来的工作。

图1.2说明了我们研究的条件，并显示了对一个简单任务的几张照片的学习，该任务要求模型从一个词中删除无关的符号。模型的性能随着自然语言任务描述的增加而提高，也随着模型上下文中的例子数量K的增加而提高。虽然这种情况下的结果特别引人注目，但模型规模和上下文中的例子数量的一般趋势在我们研究的大多数任务中都适用。我们强调，这些 "学习 "曲线不涉及梯度更新或微调，只是作为条件的示范数量增加。

大体上，在NLP任务中，GPT-3在零次和一次设置中取得了有希望的结果，在少数次设置中，有时与最先进的技术竞争，甚至偶尔超过最先进的技术（尽管最先进的技术是由微调模型保持的）。例如，GPT-3在0-shot设置中实现了81.5 F1的CoQA，在one-shot设置中实现了84.0 F1的CoQA，在 few-shot设置中实现了85.0 F1。同样，GPT-3在TriviaQA上实现了64.3%的准确率，在单次测试中实现了68.0%，在少数次测试中实现了71.2%，最后一次测试相对于在相同的闭卷测试中运行的微调模型是最先进的。

GPT-3在测试快速适应或即时推理的任务中也显示出一次和几次的熟练程度，这些任务包括解读单词、进行算术，以及在看到新词的定义后在句子中使用这些词。我们还表明，在几张照片的设置中，GPT-3可以生成合成的新闻文章，而人类评价者很难将其与人类生成的文章区分开来。

同时，我们也发现一些任务，即使在GPT-3的规模上，也有少数人的性能在挣扎。这包括自然语言推理任务，如ANLI数据集，以及一些阅读理解数据集，如RACE或QuAC。通过对GPT-3的优点和缺点的广泛描述，包括这些局限性，我们希望能刺激对语言模型中的少量学习的研究，并引起对最需要进展的地方的注意。

从图1.3中可以看出整体结果的启发式感觉，该图汇总了各种任务（尽管它本身不应该被看作是一个严格的或有意义的基准）。

我们还对 "数据污染 "进行了系统的研究--当在诸如Common Crawl这样的数据集上训练高容量模型时，这是一个日益严重的问题，它有可能包括来自测试数据集的内容，仅仅是因为这些内容经常存在于网络上。在本文中，我们开发了系统的工具来测量数据污染并量化其扭曲的影响。尽管我们发现数据污染对GPT-3在大多数数据集上的性能影响很小，但我们确实发现了一些数据集可能会夸大结果，我们要么不报告这些数据集的结果，要么根据其严重程度用星号加以标注。

除了上述所有情况外，我们还训练了一系列较小的模型（从1.25亿个参数到130亿个参数不等），以便在零、一和少量的设置中与GPT-3比较其性能。总的来说，对于大多数任务，我们发现在所有三种情况下，模型容量的扩展相对平稳；一个值得注意的模式是，零次、一次和几次的性能之间的差距往往随着模型容量的增加而增加，这也许表明更大的模型是更熟练的元学习者。

最后，鉴于GPT-3所展示的广泛能力，我们讨论了对偏见、公平和更广泛的社会影响的关注，并尝试对GPT-3在这方面的特点进行初步分析。

本文的其余部分组织如下。在第2节中，我们描述了我们训练GPT-3和评估它的方式和方法。第3节介绍了在零次、一次和几次拍摄的情况下对全部任务的结果。第4节讨论了数据污染的问题（训练-测试重叠）。第5节讨论了GPT-3的局限性。第6节讨论了更广泛的影响。第7节回顾了相关工作，第8节是结论。

