# 1  Introduction

近年来，在NLP系统中出现了预训练语言表征的趋势，并以越来越灵活和与任务无关的方式应用于下游转移。首先，单层表征是使用词向量学习的[MCCD13, PSM14]，并反馈给特定的任务架构，然后具有多层表征和上下文状态的RNN被用来形成更强大的表征[DL15, MBXS17, PNZtY18]（尽管仍然应用于特定的任务架构），最近，预训练的循环或转化器语言模型[VSP+17]已经被直接微调，完全消除对特定任务架构的需求[RNSS18, DCLT18, HR18]。

这最后一种范式在许多具有挑战性的NLP任务上取得了实质性的进展，如阅读理解、问题回答、文本嵌套等，并在新的架构和算法的基础上继续推进[RSR+19, LOG+19, YDY+19, LCG+19] 。然而，这种方法的一个主要限制是，虽然架构是任务无关的，但仍然需要特定任务的数据集和特定任务的微调：要在一个期望的任务上实现强大的性能，通常需要在数千到数十万个特定于该任务的例子的数据集上进行微调。消除这一限制是可取的，原因有几个。

首先，从实用的角度来看，每一项新的任务都需要一个大型的标记例子的数据集，这限制了语言模型的适用性。存在着非常广泛的可能有用的语言任务，包括从纠正语法，到生成抽象概念的例子，再到批评一个短篇故事。对于许多这样的任务来说，收集一个大型的监督训练数据集是很困难的，特别是当这个过程必须为每一个新的任务而重复。

其次，利用训练数据中的虚假关联的可能性从根本上说是随着模型的表现力和训练分布的狭窄程度而增长的。这可能会给预训练加微调模式带来问题，在这种模式下，模型被设计得很大，以便在预训练期间吸收信息，但随后在很窄的任务分布上进行微调。例如，[HLW+20]观察到较大的模型不一定在分布外有更好的泛化。有证据表明，在这种范式下实现的泛化可能很差，因为模型对训练分布过于具体，在分布外的泛化效果不好[YdC+19, MPL19]。因此，微调模型在特定基准上的表现，即使名义上是人类水平，也可能夸大了基础任务的实际表现[GSL+18, NK19]。

第三，人类不需要大量的监督数据集来学习大多数语言任务--自然语言的简短指令（例如 "请告诉我这个句子描述的是快乐的事情还是悲伤的事情"）或最多极少的示范（例如 "这里有两个人们表现勇敢的例子；请给出第三个勇敢的例子"）往往足以使人类执行一项新任务，至少达到合理的能力水平。除了指出我们目前的NLP技术在概念上的局限性外，这种适应性也有实际的好处--它允许人类在许多任务和技能之间无缝混合或切换，例如在冗长的对话中进行加法。为了发挥广泛的作用，我们希望有一天我们的NLP系统也能有这样的流畅性和通用性。

Figure 1.1 : 在无监督的预训练期间，语言模型发展了一套广泛的技能和模式识别能力。然后，它在推理时间使用这些能力来快速适应或识别所需的任务。我们使用术语 "上下文学习 "来描述这个过程的内部循环，它发生在每个序列的前向传递中。本图中的序列并不是为了代表模型在预训练期间看到的数据，而是为了表明有时在一个序列中会有重复的子任务。
Figure 1.2 : 大模型few-shot的效果更好

解决这些问题的一个潜在途径是元学习1--在语言模型的背景下，这意味着模型在训练时发展出一套广泛的技能和模式识别能力，然后在推理时使用这些能力来迅速适应或识别所需的任务（如图1.1所示）。最近的工作[RWC+19]试图通过我们所说的 "语境学习 "来做到这一点，使用预训练的语言模型的文本输入作为任务规范的形式：该模型以自然语言指令和/或任务的几个示范为条件，然后被期望通过预测接下来的内容来完成任务的进一步实例。

虽然它已经显示出一些初步的希望，但这种方法所取得的结果仍然远远不如微调--例如[RWC+19]在自然问题上只取得了4%的成绩，甚至它的55 F1 CoQa结果现在也比技术水平落后35分以上。元学习显然需要大量的改进，以便作为解决语言任务的实用方法而变得可行。

语言建模的另一个最新趋势可能提供了一条前进的道路。近年来，转化器语言模型的容量大幅增加，从1亿个参数[RNSS18]，到3亿个参数[DCLT18]，到15亿个参数[RWC+19]，到80亿个参数[SPP+19]，110亿个参数[RSR+19]，最后到170亿个参数[Tur20]。每一次增加都带来了文本合成和/或下游NLP任务的改进，而且有证据表明，与许多下游任务有很好关联的对数损失，随着规模的扩大也有平滑的改进趋势[KMH+20]。由于上下文学习涉及在模型的参数内吸收许多技能和任务，因此，上下文学习能力可能会随着规模的扩大而显示出类似的强大收益。

在本文中，我们通过训练一个1750亿参数的自回归语言模型（我们称之为GPT-3），并测量其语境中的学习能力来测试这一假设。具体来说，我们在二十多个NLP数据集上评估GPT-3，以及几个旨在测试快速适应不太可能直接包含在训练集中的任务的新任务。对于每个任务，我们在3个条件下评估GPT-3。(a) "少次学习"，即在语境中学习，我们允许在模型的语境窗口中容纳尽可能多的演示（通常是10到100次），(b) "一次学习"，我们只允许一次演示，以及(c) "零次 "学习，不允许演示，只给模型一个自然语言的指令。原则上，GPT-3也可以在传统的微调设置中进行评估，但我们将此留给未来的工作。

图1.2说明了我们研究的条件，并显示了对一个简单任务的几张照片的学习，该任务要求模型从一个词中删除无关的符号。模型的性能随着自然语言任务描述的增加而提高，也随着模型上下文中的例子数量K的增加而提高。虽然这种情况下的结果特别引人注目，但模型规模和上下文中的例子数量的一般趋势在我们研究的大多数任务中都适用。我们强调，这些 "学习 "曲线不涉及梯度更新或微调，只是作为条件的示范数量增加。

大体上，在NLP任务中，GPT-3在零次和一次设置中取得了有希望的结果，在少数次设置中，有时与最先进的技术竞争，甚至偶尔超过最先进的技术（尽管最先进的技术是由微调模型保持的）。例如，GPT-3在0-shot设置中实现了81.5 F1的CoQA，在one-shot设置中实现了84.0 F1的CoQA，在 few-shot设置中实现了85.0 F1。同样，GPT-3在TriviaQA上实现了64.3%的准确率，在单次测试中实现了68.0%，在少数次测试中实现了71.2%，最后一次测试相对于在相同的闭卷测试中运行的微调模型是最先进的。

GPT-3在测试快速适应或即时推理的任务中也显示出一次和几次的熟练程度，这些任务包括解读单词、进行算术，以及在看到新词的定义后在句子中使用这些词。我们还表明，在几张照片的设置中，GPT-3可以生成合成的新闻文章，而人类评价者很难将其与人类生成的文章区分开来。

同时，我们也发现一些任务，即使在GPT-3的规模上，也有少数人的性能在挣扎。这包括自然语言推理任务，如ANLI数据集，以及一些阅读理解数据集，如RACE或QuAC。通过对GPT-3的优点和缺点的广泛描述，包括这些局限性，我们希望能刺激对语言模型中的少量学习的研究，并引起对最需要进展的地方的注意。

从图1.3中可以看出整体结果的启发式感觉，该图汇总了各种任务（尽管它本身不应该被看作是一个严格的或有意义的基准）。

我们还对 "数据污染 "进行了系统的研究--当在诸如Common Crawl这样的数据集上训练高容量模型时，这是一个日益严重的问题，它有可能包括来自测试数据集的内容，仅仅是因为这些内容经常存在于网络上。在本文中，我们开发了系统的工具来测量数据污染并量化其扭曲的影响。尽管我们发现数据污染对GPT-3在大多数数据集上的性能影响很小，但我们确实发现了一些数据集可能会夸大结果，我们要么不报告这些数据集的结果，要么根据其严重程度用星号加以标注。

除了上述所有情况外，我们还训练了一系列较小的模型（从1.25亿个参数到130亿个参数不等），以便在零、一和少量的设置中与GPT-3比较其性能。总的来说，对于大多数任务，我们发现在所有三种情况下，模型容量的扩展相对平稳；一个值得注意的模式是，零次、一次和几次的性能之间的差距往往随着模型容量的增加而增加，这也许表明更大的模型是更熟练的元学习者。

最后，鉴于GPT-3所展示的广泛能力，我们讨论了对偏见、公平和更广泛的社会影响的关注，并尝试对GPT-3在这方面的特点进行初步分析。

本文的其余部分组织如下。在第2节中，我们描述了我们训练GPT-3和评估它的方式和方法。第3节介绍了在零次、一次和几次拍摄的情况下对全部任务的结果。第4节讨论了数据污染的问题（训练-测试重叠）。第5节讨论了GPT-3的局限性。第6节讨论了更广泛的影响。第7节回顾了相关工作，第8节是结论。


# 2 Approach
我们的基本预训练方法，包括模型、数据和训练，与[RWC+19]中描述的过程相似，模型大小、数据集大小和多样性以及训练长度都有相对直接的扩展。我们对语境中学习的使用也与[RWC+19]相似，但在这项工作中，我们系统地探索了不同的语境中学习设置。因此，我们在本节开始时明确地定义并对比了我们将评估GPT-3或原则上可以评估GPT-3的不同设置。这些设置可以被看作是位于它们倾向于依赖多少特定任务数据的光谱上。具体来说，我们可以在这个频谱上确定至少四个点（见图2.1的说明）。

- 微调（FT）是近年来最常见的方法，它包括通过对所需任务的特定监督数据集进行训练来更新预训练模型的权重。通常情况下，会使用几千到几十万个标记的例子。微调的主要优点是在许多基准上有强大的性能。主要的缺点是每项任务都需要一个新的大型数据集，有可能出现分布外的不良泛化[MPL19]，以及有可能利用训练数据的虚假特征[GSL+18, NK19]，有可能导致与人类表现的不公平比较。在这项工作中，我们没有对GPT-3进行微调，因为我们的重点是任务无关的性能，但GPT-3原则上是可以微调的，这是未来工作的一个有希望的方向。

- Few-Shot（FS）是我们在这项工作中使用的术语，指的是在推理时给模型一些任务的演示作为调节[RWC+19]，但不允许权重更新的设置。如图2.1所示，对于一个典型的数据集，一个例子有一个上下文和一个期望的完成度（例如一个英语句子和法语翻译），少数几个例子的作用是给出上下文和完成度的K个例子，然后是最后一个上下文的例子，期望模型能提供完成度。我们通常将K设置在10到100的范围内，因为这是模型的上下文窗口（nctx = 2048）可以容纳的例子数量。少量拍摄的主要优点是大大减少了对特定任务数据的需求，并减少了从大而窄的微调数据集中学习过度狭窄分布的可能性。主要的缺点是，到目前为止，这种方法的结果比最先进的微调模型差很多。另外，仍然需要少量的特定任务数据。如名称所示，这里描述的用于语言模型的几率学习与ML中其他情况下使用的几率学习有关[HYC01, VBL+16]--两者都涉及基于广泛的任务分布的学习（在这种情况下隐含在预训练数据中），然后快速适应新的任务。

- One-Shot(1S)与少数几个镜头相同，只是除了任务的自然语言描述外，只允许一个示范，如图1所示。将one-shot与 few-shot和zero-shot（下文）区分开来的原因是，它与一些任务传达给人类的方式最接近。例如，当要求人类在人类工作者服务（例如Mechanical Turk）上生成一个数据集时，通常会给出一个任务的示范。相比之下，如果不给出例子，有时就很难沟通任务的内容或格式。

- Zero-Shot（0S）与one-shot相同，只是不允许演示，只给模型一个描述任务的自然语言指令。这种方法提供了最大的便利性、潜在的稳健性，并避免了虚假的相关性（除非它们在预训练数据的大型语料库中非常广泛地出现），但也是最具挑战性的设置。在某些情况下，如果没有事先的例子，人类甚至很难理解任务的形式，所以这种设置在某些情况下是 "不公平的困难"。例如，如果有人被要求 "制作一个200米短跑的世界纪录表"，这个要求可能是模棱两可的，因为可能不清楚这个表到底应该有什么格式，或者应该包括什么内容（即使仔细澄清，也很难准确理解想要什么）。尽管如此，至少在某些情况下，零点拍摄最接近人类执行任务的方式--例如，在图2.1的翻译例子中，人类很可能只从文本指令中知道该怎么做。

## 2.1 Model and Architectures
我们使用与GPT-2[RWC+19]相同的模型和架构，包括其中描述的修改后的初始化、预规范化和可逆标记化，不同的是我们在转化器的层中使用交替的密集和局部带状的稀疏注意模式，类似于稀疏转化器[CGRS19]。为了研究ML性能对模型大小的依赖性，我们训练了8个不同大小的模型，范围从1.25亿个参数到1750亿个参数的三个数量级，最后一个是我们称之为GPT-3的模型。以前的工作[KMH+20]表明，在有足够训练数据的情况下，验证损失的扩展应该是一个平滑的幂律，作为规模的函数；训练许多不同规模的模型使我们能够测试这个假设，无论是验证损失还是下游的语言任务。表2.1显示了我们8个模型的大小和结构。这里nparams是可训练参数的总数，nlayers是层的总数，dmodel是每个瓶颈层的单元数（我们总是让前馈层的大小是瓶颈层的四倍，dff=4∗dmodel），dhead是每个注意头的尺寸。所有模型都使用nctx = 2048个标记的上下文窗口。我们沿着深度和宽度维度在GPU上划分模型，以尽量减少节点之间的数据传输。每个模型的精确架构参数是根据计算效率和跨GPU的模型布局的负载平衡来选择的。以前的工作[KMH+20]表明，在合理的大范围内，验证损失对这些参数并不强烈敏感。
## 2.2 Training Dataset
语言模型的数据集已经迅速扩大，最终在Common Crawl数据集2 [RSR+19]中构成了近一万亿字。这个数据集的规模足以训练我们最大的模型，而不需要在同一序列上更新两次。然而，我们发现，未经过滤或轻度过滤的Common Crawl版本的质量往往低于更精心策划的数据集。因此，我们采取了3个步骤来提高我们数据集的平均质量。(1)我们下载并过滤了一个基于与一系列高质量参考语料库相似性的CommonCrawl版本，(2)我们在数据集内部和之间进行了文档级别的模糊重复数据删除，以防止冗余，并保持我们所保留的验证集的完整性，作为对过度拟合的精确测量，以及(3)我们还将已知的高质量参考语料库加入训练组合，以增强CommonCrawl，增加其多样性。前两点（Common Crawl的处理）的细节在附录A中描述。对于第三点，我们加入了几个精心策划的高质量数据集，包括WebText数据集[RWC+19]的扩展版本，该数据集通过在较长的时间内刮取链接收集，并首次在[KMH+20]中描述，两个基于互联网的书籍语料（Books1和Books2）和英语维基百科。表2.2显示了我们在训练中使用的最终混合数据集。CommonCrawl数据是从覆盖2016年至2019年的每月CommonCrawl的41个碎片中下载的，在过滤前构成45TB的压缩明文，过滤后构成570GB，大约相当于4000亿字节对编码的标记。请注意，在训练期间，数据集的取样并不与它们的大小成比例，而是我们认为质量较高的数据集被更频繁地取样，例如CommonCrawl和Books2数据集在训练期间被取样不到一次，但其他数据集被取样2-3次。这基本上是接受了少量的过拟合，以换取更高质量的训练数据。
对于在广泛的互联网数据上进行预训练的语言模型，特别是有能力记忆大量内容的大型模型，一个主要的方法学问题是在预训练期间无意中看到他们的测试或开发集，从而可能污染下游任务。为了减少这种污染，我们搜索并试图消除与本文研究的所有基准的开发和测试集的任何重叠。不幸的是，过滤中的一个错误导致我们忽略了一些重叠，而且由于训练的成本，重新训练模型是不可行的。在第4节中，我们描述了剩余重叠部分的影响，在未来的工作中，我们将更积极地消除数据污染。
## 2.3 Training Process
正如在[KMH+20, MKAT18]中发现的那样，较大的模型通常可以使用较大的批处理量，但需要较小的学习率。率。我们在训练过程中测量梯度噪声规模，并使用它来指导我们对批次大小的选择[MKAT18]。表 2.1显示了我们使用的参数设置。为了在不耗尽内存的情况下训练更大的模型，我们在每个矩阵乘法中使用了混合的模型并行。为了在不耗尽内存的情况下训练更大的模型，我们在每个矩阵乘法中使用了模型并行性，并在网络的各层中使用了模型并行性。所有模型 都是在微软提供的高带宽集群中的V100 GPU上训练的。训练过程的细节 和超参数设置的细节在附录B中描述。9
## 2.4 Evaluation
对于少量学习，我们通过从该任务的训练集中随机抽取K个例子作为条件来评估评估集中的每个例子，根据任务的不同，用1或2个换行符来划定。对于LAMBADA和Storycloze来说，没有监督的训练集，所以我们从开发集中抽出调节的例子，在测试集上进行评估。对于Winograd（原始版本，而不是SuperGLUE版本），只有一个数据集，所以我们直接从它那里提取调节例子。

K可以是0到模型的上下文窗口所允许的最大数量的任何数值，对于所有模型来说，它是nctx=2048，通常适合10到100个例子。较大的K值通常但不一定更好，所以当有独立的开发集和测试集时，我们在开发集上试验几个K值，然后在测试集上运行最佳值。对于某些任务（见附录G），我们还使用自然语言提示，以补充（或者对于K=0，代替）演示。

在涉及从几个选项中选择一个正确答案的任务中（多选），我们提供了K个上下文加正确答案的例子，然后是一个只有上下文的例子，并比较每种答案的LM可能性。对于大多数任务，我们比较每个标记的可能性（对长度进行归一化），然而在少数数据集（ARC、OpenBookQA和RACE）上，我们通过对每个完成的无条件概率进行归一化，计算P（完成|语境）P（完成|答案语境），从而获得在开发集上的额外好处，其中答案语境是字符串 "Answer: "或 "A："，用于提示完成度应该是一个答案，但其他方面是通用的。

在涉及二元分类的任务上，我们给选项起了更有语义的名字（例如 "真 "或 "假"，而不是0或1），然后把任务当作多选题；我们有时也会像[RSR+19]那样对任务进行框定（详见附录G）。

在自由形式完成的任务上，我们使用与[RSR+19]相同参数的波束搜索：波束宽度为4，长度惩罚为α = 0.6。我们使用F1相似度得分、BLEU或精确匹配对模型进行评分，这取决于手头的数据集的标准。

如果测试集是公开的，我们会针对每个模型的大小和学习设置（零次、一次和几次）报告最终结果。当测试集是私有的时候，我们的模型往往太大，无法在测试服务器上适应，所以我们在开发集上报告结果。我们确实在少数数据集（SuperGLUE、TriviaQA、PiQa）上提交了测试服务器，在这些数据集上，我们能够使提交工作顺利进行，我们只提交了200B的几发结果，并报告了其他所有的开发集结果。

# 3 Results

## 3.1 Language Modeling, Cloze, and Completion Tasks

### 3.1.1 Language Modeling

LAMBADA数据集[PKL+16]测试了文本中长距离依赖关系的建模--模型被要求预测需要阅读一段上下文的句子的最后一个词。最近有人提出，语言模型的持续扩展在这个困难的基准上产生了递减的回报。[BHT+20]反思了最近两个最先进的结果（[SPP+19]和[Tur20]）之间模型规模翻倍所取得的1.5%的小改进，并认为 "继续扩大硬件和数据规模的数量级并不是前进的道路"。我们发现这条道路仍然是有希望的，在零次拍摄的情况下，GPT-3在LAMBADA上达到了76%，比以前的技术水平提高了8%。

LAMBADA也展示了少数学习的灵活性，因为它提供了一种方法来解决这个数据集通常出现的问题。尽管LAMBADA中的结尾总是句子中的最后一个词，但标准的语言模型没有办法知道这个细节。因此，它不仅为正确的结尾分配概率，也为该段的其他有效延续分配概率。这个问题在过去已经通过停顿词过滤器[RWC+19]（禁止 "延续 "词）得到部分解决。寥寥数语的设置反而使我们能够将任务 "框定 "为一个隐蔽的测试，并允许语言模型从例子中推断出恰好需要一个词的完成。我们使用以下的填空格式。

当以这种方式呈现例子时，GPT-3在少数情况下达到了86.4%的准确率，比之前的最先进水平增加了18%。我们观察到，几张照片的性能随着模型的大小而强烈改善。虽然这种设置使最小的模型的性能下降了近20%，但对于GPT-3来说，它提高了10%的准确性。最后，填空法在单次拍摄中并不有效，它的表现总是比零次拍摄的设置差。也许这是因为所有模型仍然需要几个例子来识别模式。

需要注意的是，对测试集污染的分析发现，LAMBADA数据集中的相当一部分似乎存在于我们的训练数据中--然而第4节中的分析表明对性能的影响可以忽略不计。

# 4 Measuring and Preventing Memorization Of Benchmarks

由于我们的训练数据集来自于互联网，我们的模型有可能是在一些基准测试集上训练出来的。准确检测互联网规模的数据集的测试污染是一个新的研究领域，没有既定的最佳实践。虽然训练大型模型而不调查污染是常见的做法，但鉴于预训练数据集的规模越来越大，我们认为这个问题越来越需要关注。

这种担忧不仅仅是假设性的。最早在Common Crawl数据上训练语言模型的论文之一[TL18]检测并删除了一个与他们的评估数据集重叠的训练文档。其他工作，如GPT-2[RWC+19]也进行了事后重叠分析。他们的研究相对令人鼓舞，发现虽然模型在训练和测试重叠的数据上确实表现得适度更好，但由于被污染的数据比例很小（通常只有百分之几），这并没有对报告结果产生重大影响。

GPT-3在某种程度上是在不同的制度下运作的。一方面，数据集和模型规模比GPT-2所用的大两个数量级，并且包括大量的Common Crawl，从而增加了污染和记忆的可能性。另一方面，正是由于大量的数据，即使是GPT-3 175B也没有对其训练集进行大量的过拟合，这是相对于与之重复的保留验证集而言的（图4.1）。因此，我们预计，污染可能是经常发生的，但其影响可能没有担心的那么大。

我们最初试图通过主动搜索并试图消除我们的训练数据和本文研究的所有基准的开发和测试集之间的任何重叠来解决污染问题。不幸的是，一个错误导致只能从训练数据中部分删除所有检测到的重叠。由于训练的成本，重新训练模型是不可行的。为了解决这个问题，我们详细调查了剩余检测到的重叠是如何影响结果的。

对于每个基准，我们产生一个 "干净 "的版本，删除所有可能泄露的例子，大致定义为与预训练集中的任何东西有13克重合的例子（或者当它短于13克时与整个例子重合）。我们的目标是非常保守地标记任何有可能是污染的东西，以便产生一个干净的子集，没有污染的高信心。具体程序详见附录C。

然后我们在这些干净的基准上评估GPT-3，并与原始分数进行比较。如果清洁子集上的得分与整个数据集上的得分相似，这表明即使存在污染，也不会对报告结果产生重大影响。如果清洁子集的得分较低，这表明污染可能会夸大结果。图4.2对这些结果进行了总结。尽管潜在的污染往往很高（四分之一的基准得分超过50%），但在大多数情况下，性能的变化可以忽略不计，而且我们没有看到证据表明污染水平和性能差异是相关的。我们的结论是，要么我们的保守方法大大高估了污染，要么污染对性能没有什么影响。

下面，我们将更详细地回顾一些特定的案例，在这些案例中，要么（1）模型在清洁版本上的表现明显更差，要么（2）潜在的污染非常高，这使得测量性能差异变得困难。

我们的分析标出了六组需要进一步调查的基准。词汇拼写、阅读理解（QuAC、SQuAD2、DROP）、PIQA、Winograd、语言建模任务（Wikitext任务、1BW），以及德译英。由于我们的重叠分析被设计得极为保守，我们预计它将产生一些假阳性结果。我们将每组任务的结果总结如下。

- 阅读理解。我们最初的分析将QuAC、SQuAD2和DROP中超过90%的任务实例标记为潜在的污染，其规模之大，甚至难以在一个干净的子集上测量差异。然而，经过人工检查，我们发现，对于我们检查的每一个重叠，在所有3个数据集中，源文本存在于我们的训练数据中，但问题/答案对却不存在，这意味着模型只获得背景信息，无法记住特定问题的答案。

- 德文翻译。我们发现WMT16德语-英语测试集中有25%的例子被标记为潜在的污染，相关的总效果大小为1-2 BLEU。经检查，没有一个被标记的例子包含与NMT训练数据相似的配对句子，碰撞是单语匹配，主要是新闻中讨论的事件片段。

- 反义词和变形词。回顾一下，这些任务的形式是 "alaok = koala"。由于这些任务的长度较短，我们使用2-grams进行过滤（忽略标点符号）。在检查了被标记的重叠后，我们发现它们通常不是训练集中真正的反转或解扰的实例，而是宫廷词或琐碎的解扰，例如 "kayak = kayak"。重叠的数量很小，但是去除琐碎的任务会导致难度的增加，从而产生虚假的信号。与此相关的是，符号插入任务显示出较高的重叠度，但对成绩没有影响--这是因为该任务涉及从一个词中删除非字母字符，而重叠分析本身忽略了这些字符，导致了许多虚假的匹配。

- PIQA。重叠分析将29%的例子标记为污染，并观察到清洁子集上的性能绝对下降了3个百分点（相对下降4%）。虽然测试数据集是在我们的训练集创建之后发布的，而且其标签是隐藏的，但众包数据集创建者使用的一些网页包含在我们的训练集中。我们发现，在一个25倍小的模型中，记忆能力也有类似的下降，这使我们怀疑这种转变很可能是统计上的偏差，而不是记忆；工人们复制的例子可能只是更容易。不幸的是，我们无法严格地证明这一假设。因此，我们将PIQA的结果用星号标出，以表示这种潜在的污染。

- Winograd。重叠分析标记了45%的例子，并发现在干净的子集上，性能下降了2.6%。对重叠数据点的人工检查表明，132个Winograd模式实际上存在于我们的训练集中，尽管它的呈现方式与我们向模型呈现的任务不同。虽然性能下降不大，但我们在主文中用星号标出了我们的Winograd结果。

- 语言建模。我们发现在GPT-2中测量的4个维基百科语言建模基准，加上儿童书籍测试数据集，几乎完全包含在我们的训练数据中。由于我们无法在这里可靠地提取一个干净的子集，所以我们没有报告这些数据集的结果，尽管我们在开始这项工作时打算这样做。我们注意到，宾夕法尼亚树银行由于其年代久远而未受影响，因此成为我们的主要语言建模基准。

我们还检查了污染程度较高，但对性能的影响接近于零的数据集，只是为了验证实际存在多少污染。这些数据集似乎经常包含假阳性。它们要么没有实际的污染，要么有污染，但并没有泄露任务的答案。一个明显的例外是LAMBADA，它似乎有大量真正的污染，但对性能的影响非常小，干净的子集的得分在全数据集的0.5%以内。另外，严格来说，我们的填空格式排除了最简单的记忆形式。尽管如此，由于我们在本文中在LAMBADA上取得了非常大的收益，所以在结果部分指出了潜在的污染。我们的污染分析的一个重要限制是，我们不能确定干净的子集是来自与原始数据集相同的分布。仍然有可能的是，记忆使结果膨胀，但同时又恰恰被一些统计上的偏差所抵消，导致清洁子集更容易。然而，接近零的转变的数量表明这是不可能的，而且我们也观察到小模型的转变没有明显的区别，这不太可能是记忆性的。总的来说，我们已经尽了最大努力来衡量和记录数据污染的影响，并根据严重程度来注意或直接删除有问题的结果。在设计基准和训练模型时，要解决这个对整个领域来说重要而微妙的问题，还有很多工作要做。关于我们分析的更详细的解释，我们请读者参考附录C。

# 5 Limitations
GPT-3和我们对它的分析有一些限制。下面我们描述其中的一些，并提出未来工作的方向。首先，尽管GPT-3在数量和质量上都有很大的改进，特别是与它的直接前身GPT-2相比，它在文本合成和几个NLP任务中仍然有明显的弱点。在文本合成方面，虽然整体质量很高，但GPT-3样本有时仍会在文档层面上重复自己的语义，在足够长的段落中开始失去连贯性，自相矛盾，并偶尔包含非连续的句子或段落。我们将发布一个由500个未经整理的无条件样本组成的集合，以帮助更好地了解GPT-3在文本合成方面的限制和优势。在离散语言任务领域，我们非正式地注意到，GPT-3似乎在 "常识性物理 "方面有特殊困难，尽管在一些测试这一领域的数据集（如PIQA [BZB+19]）上表现良好。具体来说，GPT-3对 "如果我把奶酪放进冰箱，它会融化吗？"这类问题有困难。从数量上看，GPT-3的语境学习性能在我们的一套基准上有一些明显的差距，如第3节所述，特别是在一些 "比较 "任务上，如确定两个词在一个句子中的使用方式是否相同，或一个句子是否暗示另一个句子（分别为WIC和ANLI），以及在阅读理解任务的一个子集上，它的评价比机会好不了多少。鉴于GPT-3在许多其他任务上的强大的几率性能，这一点尤其引人注目。GPT-3有几个结构和算法上的限制，这可能是上述一些问题的原因。我们专注于探索自回归语言模型中的语境学习行为，因为用这种模型类别进行采样和计算似然是很简单的。

因此，我们的实验不包括任何双向架构或其他训练目标，如去噪。这与最近的许多文献有明显的不同，这些文献记录了在使用这些方法时比标准语言模型有更好的微调性能[RSR+19]。因此，我们的设计决定的代价是，在经验上受益于双向性的任务上，性能可能会更差。这可能包括填空任务、涉及回顾和比较两段内容的任务，或者需要重读或仔细考虑一个长的段落然后产生一个非常短的答案的任务。这可能是对GPT-3在一些任务中落后的几项表现的解释，如WIC（涉及比较一个词在两个句子中的使用）、ANLI（涉及比较两个句子，看一个是否暗示另一个），以及一些阅读理解任务（如QuAC和RACE）。根据过去的文献，我们还猜想，一个大型的双向模型在微调方面会比GPT-3更强。在GPT-3的规模上制作一个双向模型，和/或尝试使双向模型与少数或零次学习一起工作，是未来研究的一个有希望的方向，可以帮助实现 "两个世界的最佳"。本文所描述的一般方法--扩大任何类似LM的模型，无论是自回归的还是双向的--的一个更根本的限制是，它最终可能会遇到（或者可能已经遇到）预训练目标的限制。我们目前的目标对每个标记的权重是相同的，并且缺乏一个概念，即什么是最重要的预测，什么是不重要的。[RRS20]展示了对感兴趣的实体进行自定义预测的好处。另外，在自我监督的目标下，任务规范依赖于强迫所需的任务进入预测问题，而最终，有用的语言系统（例如虚拟助手）可能更好地被认为是采取目标导向的行动，而不仅仅是做出预测。最后，大型预训练的语言模型并不以其他领域的经验为基础，如视频或现实世界的物理交互，因此缺乏大量的关于世界的背景[BHT+20]。由于所有这些原因，纯粹的自我监督预测的扩展可能会遇到限制，而用不同的方法进行增强可能是必要的。这方面有希望的未来方向可能包括向人类学习目标函数[ZSW+19a]，用强化学习进行微调，或增加额外的模式，如图像，以提供基础和更好的世界模型[CLY+19]。语言模型普遍存在的另一个限制是预训练时的采样效率低。虽然GPT-3在测试时的采样效率上迈出了一步，更接近于人类的采样效率（单次或零次），但它在预训练期间看到的文本仍然比人类在其一生中看到的多得多[Lin20]。提高预训练的样本效率是未来工作的一个重要方向，它可能来自于在物理世界的基础上提供额外的信息，或者来自于算法的改进。与GPT-3中的几率学习相关的一个限制，或者至少是不确定性，是关于几率学习是否真的在推理时 "从头开始 "学习新的任务，或者它是否只是识别和确定它在训练期间学到的任务。这些可能性存在于一个谱系中，从训练集中的演示与测试时的演示完全相同的分布，到识别相同的任务但以不同的形式，到适应一般任务（如QA）的特定风格，到完全从头学习一项技能。

GPT-3在这个光谱上的位置也可能因任务不同而不同。合成任务，如填词或定义无意义的词，似乎特别有可能从头开始学习，而翻译显然必须在预训练中学习，尽管可能来自于与测试数据在组织和风格上非常不同的数据。最终，我们甚至不清楚人类从零开始学习什么，还是从先前的演示中学习。即使是在预训练中组织不同的演示，并在测试时识别它们，也是语言模型的一个进步，但尽管如此，准确地理解少数几个镜头的学习是未来研究的一个重要的未探索的方向。与GPT-3规模的模型相关的一个限制是，无论目标函数或算法如何，它们既昂贵又不方便进行推理，这可能对目前形式的这种规模的模型的实际适用性构成了挑战。解决这个问题的一个可能的未来方向是将大型模型提炼[HVD15]，使其达到特定任务的可控规模。像GPT-3这样的大型模型包含了非常广泛的技能，其中大部分技能在特定任务中是不需要的，这表明原则上积极的蒸馏是可能的。蒸馏法在一般情况下被很好地探索[LHCG19a]，但还没有在千亿级参数的规模上尝试过；将其应用于这种规模的模型可能会有新的挑战和机遇。最后，GPT-3与大多数深度学习系统有一些共同的局限性--它的决定不容易解释，它对新的输入的预测不一定有很好的校准，正如在标准基准上观察到的比人类高得多的性能差异，而且它保留了它所训练的数据的偏见。最后一个问题--数据中的偏见可能导致模型产生刻板印象或偏见的内容--从社会角度看是特别值得关注的，并将在下一节更广泛的影响（第6节）中与其他问题一起讨论。

