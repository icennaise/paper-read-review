2.3
由于我们的预训练数据集的大规模，过拟合不是一个主要的问题，并且训练 CLIP 的细节相对于 Zhang 等人(2020)的实现被简化了。我们从头开始训练 CLIP，而不用 ImageNet 权重初始化图像编码器或预先训练权重的文本编码器。我们不使用表示空间和对比嵌入空间之间的非线性投影，这一变化是由 Bach-man 等人(2019)引入的，并由 Chen 等人(2020b)推广。相反，我们只使用一个线性投影来映射从每个编码器的表示到多模态嵌入空间。我们没有注意到两种方法在训练效率上的差异，并推测非线性投影可能只适用于自监督表示学习方法中的当前图像细节。我们还删除了张等人(2020)的文本转换函数 tu，该函数从文本中抽取一个统一的句子，因为 CLIP 的预训练数据集中的许多(图像，文本)对只是一个单独的句子。并对图像变换函数 tv 进行了简化。在训练过程中，唯一使用的数据增强方法是从调整大小的图像中进行随机方块裁剪。最后，在训练过程中直接将温度参数作为对数参数化乘法标量进行优化，避免了作为超参数转化。

2.4. Choosing and Scaling a Model
我们考虑了两种不同的图像编码体系结构。首先，我们使用 ResNet-50(He 等人，2016a)作为图像编码器的基本架构，因为它被广泛采用，并且性能得到验证。我们使用来自 He 等人(2019)的 ResNet-d 改进和来自 Zhang (2019)的反锯齿 rect-2模糊集对原始版本进行了多处修改。我们还将全球平均池层替换为注意力集中机制。注意力集中被实现为一个单层的“变压器式”多头 QKV 注意力，其中查询的条件是全局平均池化图像的表示。对于第二种架构，我们使用最近推出的 Vision Transformer (ViT)(Dosovitskiy et al. ，2020)进行实验。我们密切关注它们的实现，只对变压器前的组合补丁和位置嵌入添加一个额外的层标准化，并使用略有不同的初始化方案。

文本编码器是一个变压器(Vaswani 等人，2017年)与架构修改描述在雷德福等人(2019年)。作为基本尺寸，我们使用了一个63m 参数12层512宽的8个注意头模型。反转变压器工作在一个小字节对编码字母表示(BPE)的文本与49,152词汇大小(Sen-nrich 等人，2015年)。为了提高计算效率，最大序列长度被限制在76。将文本序列置于[ SOS ]和[ EOS ]标记的括号中，并将[ EOS ]标记处最高层的动作作为文本的特征表示，对文本进行层次规范化，然后线性投影到多模态嵌入空间中。在文本编码器中使用了掩蔽自我注意，以保持使用预先训练好的语言模型进行 ini-tialize 或添加语言建模作为辅助目标的能力，不过这方面的探索将留待以后进行。

虽然以前的计算机视觉研究经常通过增加独立的宽度(Mahajan 等人，2018)或者深度(He 等人，2016a)来缩放模型，但是对于 ResNet 图像编码器，我们采用了 Tan & Le (2019)的方法，该方法发现在所有宽度、深度和分辨率上分配额外的计算机比只分配给模型的一个维度要好。Tan & Le (2019)调整了分配给每个维度的计算比例，以满足其有效的 net 架构，我们使用了一个简单的基线，同等地进行附加计算，以增加模型的宽度、深度和分辨率。对于文本编码器，我们只是缩放模型的宽度，以成比例的计算宽度增加的 ResNet 和不缩放的深度，因为我们发现 CLIP 的性能是不太敏感的文本编码器的容量。

2.5 训练

我们训练一系列的5个ResNets和3个Vision Transformers。对于ResNets，我们训练了一个ResNet-50，一个ResNet-101，然后又训练了3个遵循EfficientNet风格的模型扩展，并使用大约4倍、16倍和64倍于ResNet-50的计算。它们分别表示为RN50x4、RN50x16和RN50x64。对于视觉变形器，我们训练一个ViT-B/32，一个ViT-B/16，和一个ViT-L/14。我们对所有模型进行了32次训练。我们使用Adam优化器（Kingma & Ba, 2014），对所有不是增益或偏差的权重进行解耦权重衰减正则化（Loshchilov & Hutter, 2017），并使用余弦计划衰减学习率（Loshchilov & Hutter, 2016）。最初的超参数是使用网格搜索、随机搜索和手动调整的组合，在基线ResNet-50模型上训练1个 epoch时设置的。然后，由于压缩的限制，超参数被启发式地适用于较大的模型。可学习的温度参数τ被初始化为相当于0.07，来自（Wu等人，2018），并被剪切以防止对数的缩放超过100，我们发现这对于训练的稳定性是必要的。我们使用32,768个非常大的minibatch大小。混合精度（Micikevicius等人，2017）被用来加速训练和节省内存。为了节省额外的内存，使用了梯度检查点（Griewank & Walther，2000；Chen等人，2016）、半精度Adam统计（Dhariwal等人，2020）和半精度随机取整的文本编码器权重。嵌入相似性的计算也是分片进行的，每个GPU只计算其本地批次嵌入所需的配对相似性子集。最大的ResNet模型，RN50x64，在592个V100 GPU上花了18天时间来训练，而最大的Vision Transformer在256个V100 GPU上花了12天。对于ViT-L/14，我们还以更高的336像素分辨率进行预训练，以提高与FixRes相似的性能（Touvron等人，2019）。我们把这个模型称为ViT-L/14@336px。除非另有说明，本文报道的所有 "CLIP "结果都使用这个我们认为表现最好的模型。

3. Experiments
3.1. Zero-Shot Transfer
3.1.1. MOTIVATION
在计算机视觉中，零点学习通常是指在图像分类中对未见过的物体类别进行泛化的研究（Lampert等人，2009）。相反，我们在更广泛的意义上使用这个术语，研究对未见过的数据集的概括。我们将此作为执行未见过的任务的代表，正如Larochelle等人（2008）的零数据学习论文中所期望的那样。虽然无监督学习领域的许多研究都集中在机器学习系统的表征学习能力上，但我们认为研究零次转移是衡量机器学习系统的任务学习能力的一种方式。在这种观点中，一个数据集评价了在特定分布上的任务性能。然而，许多流行的计算机视觉数据集是由研究界创建的，主要是作为指导通用图像分类方法发展的基准，而不是衡量特定任务的性能。虽然说SVHN数据集衡量的是谷歌街景照片分布上的街道号码转录任务是合理的，但目前还不清楚CIFAR-10数据集衡量的是什么 "真正的 "任务。然而，很明显的是，CIFAR-10是从TinyImages（Torralba等人，2008）中提取的分布。在这些类型的数据集上，零次转移更多的是对CLIP对分布转移和领域泛化的鲁棒性的评估，而不是任务泛化。请参阅第3.3节的分析，重点在于此。

据我们所知，Visual N-Grams（Li等人，2017）首次以上述方式研究了零照转移到现有的图像分类数据集。这也是我们所知道的唯一的其他工作，研究了使用通用预训练模型的零照转移到标准图像分类数据集，并作为CLIP上下文的最佳参考点。他们的方法学习了142,806个视觉n-grams（横跨1到5个grams）的字典的参数，并使用Jelinek-Mercer平滑的差分版本优化这些n-grams，以最大化给定图像的所有文本n-grams的概率。为了进行零点转移，他们首先将数据集的每个类别名称的文本转换为其n-gram表示，然后根据他们的模型计算其概率，预测得分最高的那个。

我们专注于研究零点转移作为任务学习的评价，是受到NLP领域中展示任务学习的工作的启发。据我们所知，Liu等人（2018）首次将任务学习确定为一种 "意外的副作用"，当时一个为生成维基百科文章而训练的语言模型学会了可靠地翻译语言间的名字。虽然GPT-1（Radford等人，2018）专注于预训练作为转移学习方法，以改善监督下的微调，但它也包括一项消减研究，证明四个启发式零点转移方法的性能在预训练的过程中稳步提高，没有任何监督下的适应。这项分析是GPT-2（Radford等人，2019年）的基础，它专门研究了通过零点转移的语言模型的任务学习能力。

3.1.2. USING CLIP FOR ZERO-SHOT TRANSFER
CLIP是预先训练好的，以预测一个图像和一个文本片段是否在其数据集中配对在一起。为了进行零点分类，我们重新使用这种能力。对于每个数据集，我们使用数据集中所有类别的名称作为潜在文本配对的集合，并根据CLIP预测最可能的（图像，文本）配对。更详细一点，我们首先计算图像的特征嵌入和可能的文本集的特征嵌入，由它们各自的编码器计算。然后计算这些嵌入的余弦相似度，用一个温度参数τ进行缩放，并通过softmax归一化为一个概率分布。请注意，这个预测层是一个多叉逻辑回归分类器，具有L2归一化的输入、L2归一化的权重、无偏置和温度缩放。这样解释的话，图像编码器是计算机视觉骨干，它计算图像的特征表示，而文本编码器是一个超网络（Ha等人，2016），它根据指定类别所代表的视觉概念的文本，生成线性分类器的权重。Lei Ba等人（2015年）首次介绍了这种形式的零拍图像分类器，而从自然语言生成分类器的想法至少可以追溯到Elhoseiny等人（2013年）。继续这种解释，CLIP预训练的每一步都可以被视为优化随机创建的计算机视觉数据集的代理性能，该数据集每类包含1个例子，共有32,768个通过自然语言描述定义的类别。对于零点评估，一旦零点分类器被文本编码器计算出来，我们就把它缓存起来，并在所有后续的预测中重新使用。这使得生成它的成本可以在数据集中的所有预测中摊销。

3.1.3. INITIAL COMPARISON TO VISUAL N-GRAMS
在表1中，我们将Visual N-Grams与CLIP进行比较。最好的CLIP模型在ImageNet上的准确率从概念证明的11.5%提高到76.2%，并与原始ResNet-50的性能相匹配，尽管没有使用这个数据集的128万个人群标记的训练实例。此外，CLIP模型的前5名准确率明显高于其前1名，该模型的前5名准确率为95%，与Inception-V4（Szegedy等人，2016）相匹配。在零照设置中，能够与强大的完全监督基线的性能相匹配，表明CLIP是朝着灵活和实用的零照计算机视觉分类器迈出的重要一步。如上所述，与Visual N-Grams的比较是为了了解CLIP的性能，不应该被解释为CLIP和Visual N-Grams之间的直接方法比较，因为这两个系统之间的许多性能相关的差异没有被控制。例如，我们在一个大10倍的数据集上进行训练，使用一个视觉模型，每次预测需要近100倍的计算量，可能使用了超过1000倍的训练计算量，并使用一个基于变压器的模型，这在Visual N-Grams发表时并不存在。作为一个更密切的比较，我们在Visual N-Grams训练的相同的YFCC100M数据集上训练了一个CLIP ResNet-50，并发现它与他们报告的ImageNet性能在V100 GPU天内相符。这个基线也是从头开始训练的，而不是像Visual N-Grams那样从预训练的ImageNet权重中初始化。
在其他两个报告的数据集上，CLIP也优于Visual N-Grams。在aYahoo上，CLIP实现了95%的错误数量的减少，而在SUN上，CLIP比Visual N-Grams的准确性高出一倍多。为了进行更全面的分析和压力测试，我们实施了一个更大的评估套件，详见附录A。总的来说，我们从Visual NGrams中报告的3个数据集扩展到包括30多个数据集，并与50多个现有的计算机视觉系统进行比较，以确定结果的背景。

3.1.4. PROMPT ENGINEERING AND ENSEMBLING
大多数标准的图像分类数据集把命名或描述类别的信息作为事后的考虑，这使得基于自然语言的零点转移成为可能。绝大多数数据集只用标签的数字ID来注释图像，并包含一个将这些ID映射到其英文名称的文件。一些数据集，如Flowers102和GTSRB，在其发布的版本中似乎根本不包括这种映射，完全阻止了零照传输。2对于许多数据集，我们观察到这些标签可能是随意选择的，并没有预见到与零照传输有关的问题，因为零照传输依赖于任务描述，以便成功传输。
一个常见的问题是多义词。当一个类的名称是提供给CLIP文本编码器的唯一信息时，由于缺乏上下文，它无法区分哪个词的意义。在某些情况下，同一个词的多种含义可能被包括在同一个数据集中的不同类别中！这种情况发生在ImageNet中，它包含了两种不同的含义。这种情况发生在ImageNet中，其中包含了建筑用起重机和飞行用起重机。另一个例子是在Oxford-IIIT宠物数据集中的类中发现的，从上下文来看，拳击手这个词显然是指一种狗的品种，但对于缺乏上下文的文本编码器来说，它也可能是指一种运动员。
我们遇到的另一个问题是，在我们的预训练数据集中，与图像配对的文本只有一个词，这是比较罕见的。通常，文本是一个完整的句子，以某种方式描述图像。为了帮助弥补这一分布差距，我们发现使用提示模板 "一张{标签}的照片。"是一个很好的默认值，有助于指定文本是关于图像的内容。这通常比只使用标签文本的基线提高了性能。例如，仅仅使用这个提示就使ImageNet的准确性提高了1.3%。
与围绕GPT3的 "提示工程 "讨论类似（Brown等人，2020；Gao等人，2020），我们也观察到，通过为每项任务定制提示文本，可以显著提高零射的性能。以下是一些非详尽的例子。我们发现在几个细粒度的图像分类数据集上，指定类别是有帮助的。例如，在Oxford-IIIT Pets上，使用 "A photo of a {label}, a type of pet. "来帮助提供背景，效果不错。同样地，在Food101上指定食物的类型和在FGVC上指定飞机的类型也有帮助。对于OCR数据集，我们发现在要识别的文本或数字周围加上引号可以提高性能。最后，我们发现，在卫星图像分类数据集上，说明图像是这种形式的，有助于我们使用"{标签}的卫星照片 "的变体。
我们还试验了将多个零星分类器集合起来作为提高性能的另一种方式。这些分类器是通过使用不同的上下文提示来计算的，比如 "一张大{标签}的照片 "和 "一张小{标签}的照片"。我们在嵌入空间而不是概率空间上构建集合。这使得我们可以缓存一组平均的文本嵌入，这样，当对许多预测进行摊销时，合集的计算成本与使用单个分类器是一样的。我们观察到，在许多生成的零次分类器中进行合集，可以可靠地提高性能，并将其用于大多数数据集。在ImageNet上，我们集合了80个不同的上下文提示，这比上面讨论的单一默认提示的性能又提高了3.5%。当一起考虑的时候，提示工程和集合提高了ImageNet的准确率近5%。在图4中，我们直观地看到，与Li等人（2017）所做的直接嵌入类名的无语境基线方法相比，提示工程和集合如何改变一组CLIP模型的性能。
