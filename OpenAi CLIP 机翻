2.3
由于我们的预训练数据集的大规模，过拟合不是一个主要的问题，并且训练 CLIP 的细节相对于 Zhang 等人(2020)的实现被简化了。我们从头开始训练 CLIP，而不用 ImageNet 权重初始化图像编码器或预先训练权重的文本编码器。我们不使用表示空间和对比嵌入空间之间的非线性投影，这一变化是由 Bach-man 等人(2019)引入的，并由 Chen 等人(2020b)推广。相反，我们只使用一个线性投影来映射从每个编码器的表示到多模态嵌入空间。我们没有注意到两种方法在训练效率上的差异，并推测非线性投影可能只适用于自监督表示学习方法中的当前图像细节。我们还删除了张等人(2020)的文本转换函数 tu，该函数从文本中抽取一个统一的句子，因为 CLIP 的预训练数据集中的许多(图像，文本)对只是一个单独的句子。并对图像变换函数 tv 进行了简化。在训练过程中，唯一使用的数据增强方法是从调整大小的图像中进行随机方块裁剪。最后，在训练过程中直接将温度参数作为对数参数化乘法标量进行优化，避免了作为超参数转化。

2.4. Choosing and Scaling a Model
我们考虑了两种不同的图像编码体系结构。首先，我们使用 ResNet-50(He 等人，2016a)作为图像编码器的基本架构，因为它被广泛采用，并且性能得到验证。我们使用来自 He 等人(2019)的 ResNet-d 改进和来自 Zhang (2019)的反锯齿 rect-2模糊集对原始版本进行了多处修改。我们还将全球平均池层替换为注意力集中机制。注意力集中被实现为一个单层的“变压器式”多头 QKV 注意力，其中查询的条件是全局平均池化图像的表示。对于第二种架构，我们使用最近推出的 Vision Transformer (ViT)(Dosovitskiy et al. ，2020)进行实验。我们密切关注它们的实现，只对变压器前的组合补丁和位置嵌入添加一个额外的层标准化，并使用略有不同的初始化方案。

文本编码器是一个变压器(Vaswani 等人，2017年)与架构修改描述在雷德福等人(2019年)。作为基本尺寸，我们使用了一个63m 参数12层512宽的8个注意头模型。反转变压器工作在一个小字节对编码字母表示(BPE)的文本与49,152词汇大小(Sen-nrich 等人，2015年)。为了提高计算效率，最大序列长度被限制在76。将文本序列置于[ SOS ]和[ EOS ]标记的括号中，并将[ EOS ]标记处最高层的动作作为文本的特征表示，对文本进行层次规范化，然后线性投影到多模态嵌入空间中。在文本编码器中使用了掩蔽自我注意，以保持使用预先训练好的语言模型进行 ini-tialize 或添加语言建模作为辅助目标的能力，不过这方面的探索将留待以后进行。

虽然以前的计算机视觉研究经常通过增加独立的宽度(Mahajan 等人，2018)或者深度(He 等人，2016a)来缩放模型，但是对于 ResNet 图像编码器，我们采用了 Tan & Le (2019)的方法，该方法发现在所有宽度、深度和分辨率上分配额外的计算机比只分配给模型的一个维度要好。Tan & Le (2019)调整了分配给每个维度的计算比例，以满足其有效的 net 架构，我们使用了一个简单的基线，同等地进行附加计算，以增加模型的宽度、深度和分辨率。对于文本编码器，我们只是缩放模型的宽度，以成比例的计算宽度增加的 ResNet 和不缩放的深度，因为我们发现 CLIP 的性能是不太敏感的文本编码器的容量。

2.5 训练

我们训练一系列的5个ResNets和3个Vision Transformers。对于ResNets，我们训练了一个ResNet-50，一个ResNet-101，然后又训练了3个遵循EfficientNet风格的模型扩展，并使用大约4倍、16倍和64倍于ResNet-50的计算。它们分别表示为RN50x4、RN50x16和RN50x64。对于视觉变形器，我们训练一个ViT-B/32，一个ViT-B/16，和一个ViT-L/14。我们对所有模型进行了32次训练。我们使用Adam优化器（Kingma & Ba, 2014），对所有不是增益或偏差的权重进行解耦权重衰减正则化（Loshchilov & Hutter, 2017），并使用余弦计划衰减学习率（Loshchilov & Hutter, 2016）。最初的超参数是使用网格搜索、随机搜索和手动调整的组合，在基线ResNet-50模型上训练1个 epoch时设置的。然后，由于压缩的限制，超参数被启发式地适用于较大的模型。可学习的温度参数τ被初始化为相当于0.07，来自（Wu等人，2018），并被剪切以防止对数的缩放超过100，我们发现这对于训练的稳定性是必要的。我们使用32,768个非常大的minibatch大小。混合精度（Micikevicius等人，2017）被用来加速训练和节省内存。为了节省额外的内存，使用了梯度检查点（Griewank & Walther，2000；Chen等人，2016）、半精度Adam统计（Dhariwal等人，2020）和半精度随机取整的文本编码器权重。嵌入相似性的计算也是分片进行的，每个GPU只计算其本地批次嵌入所需的配对相似性子集。最大的ResNet模型，RN50x64，在592个V100 GPU上花了18天时间来训练，而最大的Vision Transformer在256个V100 GPU上花了12天。对于ViT-L/14，我们还以更高的336像素分辨率进行预训练，以提高与FixRes相似的性能（Touvron等人，2019）。我们把这个模型称为ViT-L/14@336px。除非另有说明，本文报道的所有 "CLIP "结果都使用这个我们认为表现最好的模型。

3. Experiments
3.1. Zero-Shot Transfer
3.1.1. MOTIVATION
在计算机视觉中，零点学习通常是指在图像分类中对未见过的物体类别进行泛化的研究（Lampert等人，2009）。相反，我们在更广泛的意义上使用这个术语，研究对未见过的数据集的概括。我们将此作为执行未见过的任务的代表，正如Larochelle等人（2008）的零数据学习论文中所期望的那样。虽然无监督学习领域的许多研究都集中在机器学习系统的表征学习能力上，但我们认为研究零次转移是衡量机器学习系统的任务学习能力的一种方式。在这种观点中，一个数据集评价了在特定分布上的任务性能。然而，许多流行的计算机视觉数据集是由研究界创建的，主要是作为指导通用图像分类方法发展的基准，而不是衡量特定任务的性能。虽然说SVHN数据集衡量的是谷歌街景照片分布上的街道号码转录任务是合理的，但目前还不清楚CIFAR-10数据集衡量的是什么 "真正的 "任务。然而，很明显的是，CIFAR-10是从TinyImages（Torralba等人，2008）中提取的分布。在这些类型的数据集上，零次转移更多的是对CLIP对分布转移和领域泛化的鲁棒性的评估，而不是任务泛化。请参阅第3.3节的分析，重点在于此。
