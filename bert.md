# 1 Introduction
目前已有的两种通过预训练语言表征再应用至下游任务的策略
### 1: 基于特征的策略
代表：ELMo。使用任务专用的模型架构，将语言表征作为一个额外的特征输入。
### 2: 微调的策略（fine-tuning）
代表：GPT，尽量少地引入任务专用的参数，通过微调所有预训练参数对下游任务进行训练。



作者指出不足：单向的语言模型限制了表现，在句子级的任务上“不是最优”，在fine-tuning方式的token级的任务上“十分有害”

贡献：

1.对比单向网络和双向单独训练再串联，Bert的双向效果更好

2.证明预训练表征可以减少对特定任务模型的依赖，Bert作为微调模型，在句子级和token级任务的表现能超过一些特定任务模型


# 2.1 Related Work
## 2.1 无监督的基于特征类方法
预训练word embedding

预训练句子embedding、或段落embedding

ELMo:双向提取word embedding，根据下游任务训练权重。
## 2.2 无监督的微调类方法
早期只有word embedding

现在出现了利用句子、文档生成的上下文token表征。它利用无标签的数据训练，通过有监督的下游任务微调。但是，是单向的。
## 2.3 迁移学习

# 3 Bert
框架有两步：预训练和微调。

预训练：模型通过无标签数据训练参数。

微调：模型使用预训练的参数，通过下游任务的标签微调模型。

针对不同的下游任务，使用同一套预训练参数，分别进行微调。

#### 统一架构 
特点是，Bert应用于不同的任务上后，和预训练的架构很相似。
