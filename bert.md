# 1 Introduction
目前已有的两种通过预训练语言表征再应用至下游任务的策略
### 1: 基于特征的策略
代表：ELMo。使用任务专用的模型架构，将语言表征作为一个额外的特征输入。
### 2: 微调的策略（fine-tuning）
代表：GPT，尽量少地引入任务专用的参数，通过微调所有预训练参数对下游任务进行训练。



作者指出不足：单向的语言模型限制了表现，在句子级的任务上“不是最优”，在fine-tuning方式的token级的任务上“十分有害”

贡献：

1.对比单向网络和双向单独训练再串联，Bert的双向效果更好

2.证明预训练表征可以减少对特定任务模型的依赖，Bert作为微调模型，在句子级和token级任务的表现能超过一些特定任务模型


# 2 Related Work
## 2.1 无监督的基于特征类方法
预训练word embedding

预训练句子embedding、或段落embedding

ELMo:双向提取word embedding，根据下游任务训练权重。
## 2.2 无监督的微调类方法
早期只有word embedding

现在出现了利用句子、文档生成的上下文token表征。它利用无标签的数据训练，通过有监督的下游任务微调。但是，是单向的。
## 2.3 迁移学习

# 3 Bert

![image](https://user-images.githubusercontent.com/21083001/147463866-02c81396-9f61-4c98-ad0c-29c834edf225.png)

框架有两步：预训练和微调。

预训练：模型通过无标签数据训练参数。

微调：模型使用预训练的参数，通过下游任务的标签微调模型。

针对不同的下游任务，使用同一套预训练参数，分别进行微调。

#### 统一架构 
特点是，Bert应用于不同的任务上后，和预训练的架构很相似。

Bert有两个版本，小的用于和参数数量相当的OpenAiGPT对比，大的当然是刷榜去了

#### 输入输出
由于Bert使用Transoformer中的encoder堆叠，encoder的输入输出维度是一样的，可以将输出看作是输入经过学习的表征。

输入可以是一个“句子”（这里的句子不同于我们说的句子，可以很短只有一个词甚至一个字母，也可以很长是一篇文章），也可以是一个“句子”对。

有两个特殊符号，【CLS】,【SEP】。

前者放在每个输入的开头，对应的输出被用作分类任务，可以看作是整个输入的聚合表征。

后者是当输入是一个“句子”对时，这个符号放在两个句子对中间。

相比transformer，Bert为属于不同的句子的序列增加了一个segment embeddings，和【SEP】符号一起工作，将句子对区别开来。

![image](https://user-images.githubusercontent.com/21083001/147464430-ebe2a81c-2984-49ae-8f9a-7d50f92abd18.png)


## 3.1 Pre-training BERT
显然，深度双向语言模型表现肯定强于单向或浅层双向串联模型，但是如果直接尝试训练双向语言模型，会导致每个词间接地看到“自己”。

注：我认为类似与transformer中decoder环节需要对第一层输入加mask防止decoder预先看到未来的词，模型训练的一个原则就是模型训练的过程要和真正预测推理的过程保持一致。如果在训练时不mask，那就相当在用整个ground truth去预测下一个单词，而实际上decoder是逐个预测下一个词而不能知道“未来的词”的。

？轻而易举地通过多层上下文预测目标词？这意味着模型不能将做得很深？

如果我们先后从左侧和右侧同时去训练一个语言模型，那么后一次训练时实际上模型已经被告知完整的句子了。

增加难度，为了训练更深的模型：随机屏蔽15%的输入，替换为特殊
